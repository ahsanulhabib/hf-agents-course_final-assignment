{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabc825f",
   "metadata": {},
   "source": [
    "# OCR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "812a2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import io\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e241d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(\n",
    "    image_path: str,\n",
    "    prompt: Optional[str] = \"Extract all readable text from this image.\",\n",
    "    api_key: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract text from an image using Gemini 2.5 Pro Vision multimodal capabilities via LangChain.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        prompt (str): Prompt to guide Gemini in extracting text.\n",
    "        api_key (str): Your Google API key with Gemini access.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text response from Gemini.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    with Image.open(image_path) as img:\n",
    "        img_bytes = io.BytesIO()\n",
    "        img.save(img_bytes, format=\"PNG\")\n",
    "        img_bytes.seek(0)\n",
    "        image_base64 = base64.b64encode(img_bytes.read()).decode(\"utf-8\")\n",
    "\n",
    "        message = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Extract all the text from this image. \"\n",
    "                        \"Return only the extracted text, no explanations.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    if api_key is None:\n",
    "        raise ValueError(\"You must provide a Google Gemini API key.\")\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro-exp-03-25\",\n",
    "        google_api_key=api_key,\n",
    "        convert_system_message_to_human=True,\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(message)\n",
    "\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \".\\\\src\\\\gaia_test\\\\data\\\\sample_ocr_image.png\"\n",
    "GOOGLE_API_KEY = \"your-google-api-key\"\n",
    "\n",
    "# List available models to debug the error\n",
    "models = genai.list_models()\n",
    "print(\"Available models:\")\n",
    "for model in models:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "feda88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahabib\\OneDrive - Aged Care Quality and Safety Commission\\Documents\\Projects\\Codes\\AgenticWorkflow\\hf-agents-course_final-assignment\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " Lorem ips's deplace\n",
      "-Lorem,-ipsum dolor sit as a consectindiarm, consectresum, indisim\n",
      "incildis me dolore-illant, quis appristenched labor, filsit in labor\n",
      "magna, amil jouri paratum.\n",
      "-A em-sim, occasentdis tempar cublling dota, consentables\n",
      "noisucle disfirycliplor vise, ant Emadisalitais sild befure posscat\n",
      "in at depececting hlis molcicat, tempers desforentius lean compdntr,\n",
      "for the magnants thic procedients.\n",
      "-A at perfermtimg the llaventor ate.\n",
      "-A perum dutis may minim placere, et culdnt, blals, temporam places\n",
      "vollurat paceal supper the diffectates, quis,\n",
      "caperour decinsum on the offpeate maginam.\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_from_image(IMAGE_PATH, api_key=GOOGLE_API_KEY)\n",
    "print(\"Extracted Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f95ff",
   "metadata": {},
   "source": [
    "# YT-DLP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b6b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import yt_dlp\n",
    "import contextlib\n",
    "import json\n",
    "from pprint import pprint\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def extract_youtube_metadata(video_url):\n",
    "    \"\"\"\n",
    "    Extract metadata from a YouTube video using yt-dlp.\n",
    "\n",
    "    Args:\n",
    "        video_url (str): The URL of the YouTube video.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metadata dictionary for the video.\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'skip_download': True,\n",
    "        'extract_flat': False,\n",
    "    }\n",
    "    ydl_opts = {\n",
    "                \"quiet\": True,\n",
    "                \"no_warnings\": True,\n",
    "                \"extract_flat\": \"in_playlist\",\n",
    "                \"forcejson\": True,\n",
    "                \"skip_download\": True,\n",
    "                \"youtube_include_dash_manifest\": False,\n",
    "            }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        video_info = ydl.extract_info(video_url, download=False)\n",
    "        buffer = io.StringIO()\n",
    "        with contextlib.redirect_stdout(buffer):\n",
    "            ydl.extract_info(video_url, download=False)\n",
    "            \n",
    "        # extracted_info_json = info # buffer.getvalue()\n",
    "        \n",
    "        # if not extracted_info_json:\n",
    "        #     return f\"Error: yt-dlp returned no metadata for {video_url}.\"\n",
    "\n",
    "        # first_video_info = None\n",
    "        \n",
    "        # for line in extracted_info_json.strip().split(\"\\n\"):\n",
    "        #     try:\n",
    "        #         first_video_info = json.loads(line)\n",
    "        #         break\n",
    "        #     except json.JSONDecodeError:\n",
    "        #         continue\n",
    "        # if not first_video_info:\n",
    "        #     return (\n",
    "        #         f\"Error: Could not parse metadata JSON for {video_url}.\"\n",
    "        #     )\n",
    "\n",
    "        title = video_info.get(\"title\", \"N/A\")\n",
    "        if isinstance(title, str):\n",
    "            title = title.strip()\n",
    "        description = video_info.get(\"description\", \"N/A\")\n",
    "        if isinstance(description, str):\n",
    "            description = description.strip()\n",
    "        uploader = video_info.get(\"uploader\", \"N/A\")\n",
    "        if isinstance(uploader, str):\n",
    "            uploader = uploader.strip()\n",
    "        duration_s = video_info.get(\"duration\")\n",
    "        duration = (\n",
    "            time.strftime(\"%H:%M:%S\", time.gmtime(duration_s))\n",
    "            if duration_s\n",
    "            else \"N/A\"\n",
    "        )\n",
    "        views = video_info.get(\"view_count\", \"N/A\")\n",
    "        date = video_info.get(\"upload_date\", \"N/A\")\n",
    "        if isinstance(date, str):\n",
    "            date = date.strip()\n",
    "        if date and len(date) == 8:\n",
    "            date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "        summary = (\n",
    "            f\"Metadata for YouTube video: {video_url}\\nTitle: {title}\\nUploader: {uploader}\\n\"\n",
    "            f\"Date: {date}\\nDuration: {duration}\\nViews: {views}\\nDesc (truncated):\\n---\\n\"\n",
    "            f\"{description[:1000]}{'...' if len(description) > 1000 else ''}\\n---\\nNote: Metadata only.\"\n",
    "        )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ef3054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Metadata for YouTube video: https://www.youtube.com/watch?v=dQw4w9WgXcQ\\n'\n",
      " 'Title: Rick Astley - Never Gonna Give You Up (Official Music Video)\\n'\n",
      " 'Uploader: Rick Astley\\n'\n",
      " 'Date: 2009-10-25\\n'\n",
      " 'Duration: 00:03:32\\n'\n",
      " 'Views: 1650302083\\n'\n",
      " 'Desc (truncated):\\n'\n",
      " '---\\n'\n",
      " 'The official video for “Never Gonna Give You Up” by Rick Astley. \\n'\n",
      " '\\n'\n",
      " 'Never: The Autobiography 📚 OUT NOW! \\n'\n",
      " 'Follow this link to get your copy and listen to Rick’s ‘Never’ playlist ❤️ '\n",
      " '#RickAstleyNever\\n'\n",
      " 'https://linktr.ee/rickastleynever\\n'\n",
      " '\\n'\n",
      " '“Never Gonna Give You Up” was a global smash on its release in July 1987, '\n",
      " 'topping the charts in 25 countries including Rick’s native UK and the US '\n",
      " 'Billboard Hot 100.  It also won the Brit Award for Best single in 1988. '\n",
      " 'Stock Aitken and Waterman wrote and produced the track which was the '\n",
      " 'lead-off single and lead track from Rick’s debut LP “Whenever You Need '\n",
      " 'Somebody”.  The album was itself a UK number one and would go on to sell '\n",
      " 'over 15 million copies worldwide.\\n'\n",
      " '\\n'\n",
      " 'The legendary video was directed by Simon West – who later went on to make '\n",
      " 'Hollywood blockbusters such as Con Air, Lara Croft – Tomb Raider and The '\n",
      " 'Expendables 2.  The video passed the 1bn YouTube views milestone on 28 July '\n",
      " '2021.\\n'\n",
      " '\\n'\n",
      " 'Subscribe to the official Rick Astley YouTube channel: https://RickAstl...\\n'\n",
      " '---\\n'\n",
      " 'Note: Metadata only.')\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "metadata = extract_youtube_metadata(video_url)\n",
    "pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64012428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata for YouTube video: https://www.youtube.com/watch?v=dQw4w9WgXcQ\\nTitle: Rick Astley - Never Gonna Give You Up (Official Music Video)\\nUploader: Rick Astley\\nDate: 2009-10-25\\nDuration: 00:03:32\\nViews: 1650302083\\nDesc (truncated):\\n---\\nThe official video for “Never Gonna Give You Up” by Rick Astley. \\n\\nNever: The Autobiography 📚 OUT NOW! \\nFollow this link to get your copy and listen to Rick’s ‘Never’ playlist ❤️ #RickAstleyNever\\nhttps://linktr.ee/rickastleynever\\n\\n“Never Gonna Give You Up” was a global smash on its release in July 1987, topping the charts in 25 countries including Rick’s native UK and the US Billboard Hot 100.  It also won the Brit Award for Best single in 1988. Stock Aitken and Waterman wrote and produced the track which was the lead-off single and lead track from Rick’s debut LP “Whenever You Need Somebody”.  The album was itself a UK number one and would go on to sell over 15 million copies worldwide.\\n\\nThe legendary video was directed by Simon West – who later went on to make Hollywood blockbusters such as Con Air, Lara Croft – Tomb Raider and The Expendables 2.  The video passed the 1bn YouTube views milestone on 28 July 2021.\\n\\nSubscribe to the official Rick Astley YouTube channel: https://RickAstl...\\n---\\nNote: Metadata only.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586027e0",
   "metadata": {},
   "source": [
    "# AGENT-CORE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb178c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8bfc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of C:\\Users\\ahabi\\OneDrive\\Documents\\Codes\\Python Projects\\Huggingface - AI Agents Course\\hf-agents-course_final-assignment\\.venv\\Lib\\site-packages\\langgraph\\prebuilt:\n",
      "- chat_agent_executor.py\n",
      "- interrupt.py\n",
      "- py.typed\n",
      "- tool_node.py\n",
      "- tool_validator.py\n",
      "- __init__.py\n",
      "- __pycache__\n",
      "\n",
      "Successfully imported from langgraph.prebuilt. Available names:\n",
      "['In', 'InjectedState', 'InjectedStore', 'Out', 'ToolNode', 'ValidationNode', '_', '_1', '_2', '__', '__DW_SCOPE__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '__vsc_ipynb_file__', '_dh', '_i', '_i1', '_i2', '_i3', '_ih', '_ii', '_iii', '_oh', 'create_react_agent', 'exit', 'get_ipython', 'item', 'langgraph_path', 'open', 'os', 'quit', 'sys', 'tools_condition']\n"
     ]
    }
   ],
   "source": [
    "# Get the path to the langgraph.prebuilt directory\n",
    "langgraph_path = \"C:\\\\Users\\\\ahabi\\\\OneDrive\\\\Documents\\\\Codes\\\\Python Projects\\\\Huggingface - AI Agents Course\\\\hf-agents-course_final-assignment\\\\.venv\\\\Lib\\\\site-packages\\\\langgraph\\\\prebuilt\"\n",
    "\n",
    "# List the contents of the directory\n",
    "if os.path.isdir(langgraph_path):\n",
    "    print(f\"Contents of {langgraph_path}:\")\n",
    "    for item in os.listdir(langgraph_path):\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {langgraph_path}\")\n",
    "\n",
    "# Additionally, try importing the __init__.py file and see its contents\n",
    "try:\n",
    "    from langgraph.prebuilt import *\n",
    "    print(f\"\\nSuccessfully imported from langgraph.prebuilt. Available names:\")\n",
    "    print(dir())\n",
    "except ImportError as e:\n",
    "    print(f\"\\nError importing from langgraph.prebuilt: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51677050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InjectedState', 'InjectedStore', 'ToolNode', 'ValidationNode', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'chat_agent_executor', 'create_react_agent', 'tool_node', 'tool_validator', 'tools_condition']\n"
     ]
    }
   ],
   "source": [
    "import langgraph.prebuilt\n",
    "print(dir(langgraph.prebuilt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c47c8b",
   "metadata": {},
   "source": [
    "# Test HF API ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eda9c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahabi\\OneDrive\\Documents\\Codes\\Python Projects\\Huggingface - AI Agents Course\\hf-agents-course_final-assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available inference endpoints: []\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_inference_endpoints\n",
    "endpoints = list_inference_endpoints(namespace=\"*\")\n",
    "print(f\"Available inference endpoints: {endpoints}\")\n",
    "for endpoint in endpoints:\n",
    "    print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c84e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaia_agent.tools import get_all_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ecb26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing All Tools ---\n",
      "⚠️ Tavily Search failed (1 validation error for TavilySearchTool\n",
      "api_wrapper\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing), falling back to DuckDuckGo.\n",
      "✅ Initialized DuckDuckGo Search (Fallback)\n",
      "✅ Initialized Wikipedia Search\n",
      "✅ Initialized ArXiv Search\n",
      "❌ Failed to initialize ArXiv Document Search: \"ArxivDocumentSearchTool\" object has no field \"load_max_docs\"\n",
      "✅ Initialized File Tools (Save, Download)\n",
      "✅ Initialized Text Analysis Tool\n",
      "✅ Initialized CSV Analysis Tool\n",
      "✅ Initialized Excel Analysis Tool\n",
      "✅ Initialized Image Analysis Tool\n",
      "✅ Initialized MP3 Analysis Tool\n",
      "✅ Initialized Image OCR Tool\n",
      "✅ Initialized Python REPL Tool\n",
      "✅ Initialized YouTube Metadata Tool\n",
      "✅ Initialized Math Tools (Add, Subtract, Multiply, Divide, Modulus)\n",
      "--- Total tools initialized: 18 ---\n",
      "Available tool names: ['duckduckgo_search', 'wikipedia_search', 'arxiv_search', 'save_content_to_file', 'download_file_from_url', 'analyze_text_file', 'analyze_csv_file', 'analyze_excel_file', 'analyze_image_content', 'analyze_mp3_audio', 'extract_text_from_image', 'python_repl', 'analyze_youtube_metadata', 'add', 'subtract', 'multiply', 'divide', 'modulus']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DuckDuckGoSearchTool(api_wrapper=DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))),\n",
       " WikipediaSearchTool(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\ahabi\\\\OneDrive\\\\Documents\\\\Codes\\\\Python Projects\\\\Huggingface - AI Agents Course\\\\hf-agents-course_final-assignment\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " ArXivSearchTool(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=10, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " SaveContentTool(),\n",
       " DownloadFileTool(),\n",
       " AnalyzeTextTool(),\n",
       " AnalyzeCsvTool(),\n",
       " AnalyzeExcelTool(),\n",
       " AnalyzeImageContentTool(),\n",
       " AnalyzeMP3Tool(),\n",
       " ExtractImageTextTool(),\n",
       " PythonReplTool(),\n",
       " AnalyzeYoutubeMetadataTool(),\n",
       " AddTool(),\n",
       " SubtractTool(),\n",
       " MultiplyTool(),\n",
       " DivideTool(),\n",
       " ModulusTool()]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_tools()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
